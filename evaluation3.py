# this script compares the ground truth sketches with the sketches generated by mediapipe for real and synthetic hands
# it uses ssim, lpips, and psnr to find their differences, saves to dataframe where rows = gt sketches,
# columns = ssim-MP, lpips-MP, psnr-MP
import torchvision.transforms as transforms
from torch import permute, unsqueeze
import pandas as pd
import numpy
import math
import pdb
import os
import torch
from PIL import Image
from skimage.metrics import structural_similarity
import lpips
import cv2
from transformers import AutoProcessor, CLIPVisionModelWithProjection
import os
import numpy as np
from scipy.spatial import KDTree
from tqdm import tqdm
from models.load_sketchbert import get_sketchbert_model, prepare_input_sketch
import numpy as np
import json
from data.helpers import extract_coord_sequence_from_image
from rdp import rdp
import re
from data.helpers import generate_sketch_image
from data.prepare_datasets_for_augs2s import preprocess_video_sketch_for_validation 

held_out_categories = ["cat", "car", "face", "snail", "sun", "candle", "angel", "grapes", "cow", "diamond"]

def chamfer_distance_points(image1, image2):
    """
    Computes the chamfer distance between two sets of points A and B.
    TODO: Relook at distance transform
    """
    black_pixels = np.where(image1 == 0)
    black_pixel_coordinates1 = list(zip(black_pixels[0], black_pixels[1]))
    black_pixels = np.where(image2 == 0)
    black_pixel_coordinates2 = list(zip(black_pixels[0], black_pixels[1]))

    tree = KDTree(black_pixel_coordinates2)
    dist_A = tree.query(black_pixel_coordinates1)[0]
    tree = KDTree(black_pixel_coordinates1)
    dist_B = tree.query(black_pixel_coordinates2)[0]
    return np.mean(dist_A) + np.mean(dist_B)

def compute_distance_transform(binary_image):
    # Ensure binary image is in the correct form (foreground should be white)
    _, binary_image = cv2.threshold(binary_image, 127, 255, cv2.THRESH_BINARY)
    # Compute distance transform
    distance_transform = cv2.distanceTransform(binary_image, cv2.DIST_L2, 3)
    return distance_transform

def chamfer_distance_image(image1, image2):
    # Compute distance transforms
    dt1 = compute_distance_transform(image1)
    dt2 = compute_distance_transform(image2)
    
    chamfer12 = np.mean(dt1[image2 == 0])
    chamfer21 = np.mean(dt2[image1 == 0])
    
    return (chamfer12 + chamfer21) / 2

def get_sketch_sequence_from_image(image_array):
    if len(image_array.shape) > 2:
        image_array = image_array.mean(2)
    black_indices = np.where(image_array == 0)

    # Normalize the coordinates
    height, width = image_array.shape
    normalized_x = black_indices[1] / (width - 1)
    normalized_y = black_indices[0] / (height - 1)

    # Combine x and y into a list of tuples
    normalized_points = list(zip(normalized_x, normalized_y))
    return normalized_points

def calculate_retrieval_scores(image_list1, image_list2, model, processor):
    with torch.no_grad():
        input1 = processor(images=image_list1, return_tensors = 'pt').pixel_values.cuda()
        embeds1 = model(input1).image_embeds
        embeds1 = embeds1 / embeds1.norm(dim=1, keepdim=True)

        input2 = processor(images=image_list2, return_tensors = 'pt').pixel_values.cuda()
        embeds2 = model(input2).image_embeds
        embeds2 = embeds2 / embeds2.norm(dim=1, keepdim=True)

        sim = embeds1 @ embeds2.T

        image1to2_retrieval_1 = sim.argmax(dim=1) == torch.arange(len(image_list1)).cuda()
        return image1to2_retrieval_1.float().mean().cpu().item()


def prepare_batch(batch):
    batch = [extract_coord_sequence_from_image(img) for img in batch]
    batch = [[np.array(rdp([(point[0],point[1]) for point in stroke], 2)) for stroke in sketch] for sketch in batch]
    batch = [prepare_input_sketch(sketch) for sketch in batch]
    mask_input_states, input_states, segments, length_masks, input_masks = [], [], [], [], []
    for sketch in batch:
        mask_input_states.append(sketch[0])
        input_states.append(sketch[1])
        segments.append(sketch[2])
        length_masks.append(sketch[3])
        input_masks.append(sketch[4])

    mask_input_states = torch.stack(mask_input_states).to('cuda')
    input_states = torch.stack(input_states).to('cuda')
    segments = torch.stack(segments).to('cuda')
    length_masks = torch.stack(length_masks).to('cuda')
    input_masks = torch.stack(input_masks).to('cuda')
    return mask_input_states, input_states, segments, length_masks, input_masks

def evaluate(root, metrics=['ssim', 'lpips', 'psnr', 'chamfer_dist_points', 'chamfer_dist_images'], save_path=None):

    columns = ['filename', 'category', 'idx']
    for metric in metrics:
        columns.append(f"{metric}AUG")
        columns.append(f"{metric}AUGGen")
        columns.append(f"{metric}MP")
        columns.append(f"{metric}MPGen")

    # model = CLIPVisionModelWithProjection.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16, device_map = 'cuda')
    # model.eval()
    # processor = AutoProcessor.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16, device_map = 'cuda')
    mpPath = os.path.join(root,'mp')
    augPath = os.path.join(root,'aug') 
    gtPath = os.path.join(root,'gt')
    mpgenPath = os.path.join(root,'mp_gen')
    auggenPath = os.path.join(root, 'aug_gen')

    images_gt_all = []
    images_mp_all = []
    images_mpgen_all = []

    images_aug_all = []
    images_auggen_all = []

    results = []

    gt_vs_mp_sketchbert_mean = []
    gt_vs_gen_sketchbert_mean = []

    pattern = r'^([^0-9]+)(\d+)\.png'

    if 'lpips' in metrics:
        loss_fn_vgg = lpips.LPIPS(net='vgg')
        toTensor = transforms.ToTensor()

    for root, dirs, files in os.walk(gtPath):
        if root == gtPath: continue

        category = root.split("/")[-1]

        images_gt = []
        images_mp = []
        images_mpgen = []
        images_aug = []
        images_auggen = []

        for filename in tqdm(files, desc=f"Evaluating {category}"):
            # if filename != "beach68.png":
            #     continue
            match = re.match(pattern, filename)
            idx = match.group(2)

            rawGT = cv2.imread(os.path.join(gtPath, category, filename))
            rawAUG = cv2.imread(os.path.join(augPath, category, filename))
            rawMP = cv2.imread(os.path.join(mpPath, category, filename))
            rawAUG = 255 - rawAUG
            rawMP = 255 - rawMP
            rawAUGGen = cv2.imread(os.path.join(auggenPath, category, filename))
            rawMPGen = cv2.imread(os.path.join(mpgenPath, category, filename))

            if rawGT.shape != (256,256, 3) or rawMP.shape != (256,256, 3) or rawMPGen.shape != (256,256, 3) or rawAUG.shape != (256,256, 3) or rawAUGGen.shape != (256,256, 3):
                raise Exception(f'Find shape mismatch: {filename}')

            images_gt.append(Image.fromarray(rawGT))
            images_aug.append(Image.fromarray(rawAUG))
            images_mp.append(Image.fromarray(rawMP))
            images_auggen.append(Image.fromarray(rawAUGGen))
            images_mpgen.append(Image.fromarray(rawMPGen))

            greyGT = cv2.cvtColor(rawGT, cv2.COLOR_BGR2GRAY)
            greyAUG = cv2.cvtColor(rawAUG, cv2.COLOR_BGR2GRAY)
            greyMP = cv2.cvtColor(rawMP, cv2.COLOR_BGR2GRAY)
            greyAUGGen = cv2.cvtColor(rawAUGGen, cv2.COLOR_BGR2GRAY)
            greyMPGen = cv2.cvtColor(rawMPGen, cv2.COLOR_BGR2GRAY)

            _, binAUGGen = cv2.threshold(greyAUGGen, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            _, binMPGen  = cv2.threshold(greyMPGen, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

            result = []
    
            for metric in metrics:
                if metric == 'ssim':
                    (ssim_aug, _) = structural_similarity(greyGT, greyAUG, full=True)
                    (ssim_auggen, _) = structural_similarity(greyGT, greyAUGGen, full=True)
                    (ssim_mp, _) = structural_similarity(greyGT, greyMP, full=True)
                    (ssim_mpgen, _) = structural_similarity(greyGT, greyMPGen, full=True)
                    result.append(ssim_aug)
                    result.append(ssim_auggen)
                    result.append(ssim_mp)
                    result.append(ssim_mpgen)
                
                elif metric == 'psnr':
                    psnrAUG = cv2.PSNR(rawGT, rawAUG)
                    psnrAUGGen = cv2.PSNR(rawGT, rawAUGGen)
                    psnrMP = cv2.PSNR(rawGT, rawMP)
                    psnrMPGen = cv2.PSNR(rawGT, rawMPGen)
                    result.append(psnrAUG)
                    result.append(psnrAUGGen)
                    result.append(psnrMP)
                    result.append(psnrMPGen)

                elif metric == 'lpips':

                    tensorGT = unsqueeze(toTensor(rawGT), 0)

                    lpips_aug = loss_fn_vgg(tensorGT, toTensor(rawAUG).unsqueeze()).detach().numpy()[0][0][0][0]
                    lpips_auggen = loss_fn_vgg(tensorGT, toTensor(rawAUGGen).unsqueeze()).detach().numpy()[0][0][0][0]
                    lpips_mp = loss_fn_vgg(tensorGT, toTensor(rawMP).unsqueeze()).detach().numpy()[0][0][0][0]
                    lpips_mpgen = loss_fn_vgg(tensorGT, toTensor(rawMPGen).unsqueeze()).detach().numpy()[0][0][0][0]
                    result.append(lpips_aug)
                    result.append(lpips_auggen)
                    result.append(lpips_mp)
                    result.append(lpips_gen)

                elif metric == 'chamfer_dist_points':
                    chamfer_aug = chamfer_distance_points(greyGT, greyAUG)
                    chamfer_auggen = chamfer_distance_points(greyGT, binAUGGen)
                    chamfer_mp = chamfer_distance_points(greyGT, greyMP)
                    chamfer_mpgen = chamfer_distance_points(greyGT, binMPGen)
                    result.append(chamfer_aug)
                    result.append(chamfer_auggen)
                    result.append(chamfer_mp)
                    result.append(chamfer_mpgen)

                elif metric == 'chamfer_dist_images':
                    
                    chamfer_aug = chamfer_distance_image(greyGT, greyAUG)
                    chamfer_auggen = chamfer_distance_image(greyGT, binAUGGen)
                    chamfer_mp = chamfer_distance_image(greyGT, greyMP)
                    chamfer_mpgen = chamfer_distance_image(greyGT, binMPGen)
                    result.append(chamfer_aug)
                    result.append(chamfer_auggen)
                    result.append(chamfer_mp)
                    result.append(chamfer_mpgen)

                

            row = [filename, category, idx] + result
            results.append(row + (len(columns) - len(row)) * [None])
        
        images_gt_all.extend(images_gt)
        images_mp_all.extend(images_mp)
        images_mpgen_all.extend(images_mpgen)

    batch_size = 128

    if 'sketchbert' in metrics:
        sketchbert = get_sketchbert_model('/home/scui/projects/DoodleFusion/models/SketchBERT/qd_8_12_768_mask/latest_ckpt.pth.tar')
        sketchbert.eval()
        sketchbert = sketchbert.to('cuda')
        sketchbert_gt_mp = []
        sketchbert_gt_gen = []

        for i in range(0,len(images_mpgen_all), batch_size):
            batch_gt = images_gt_all[i:i+128]
            batch_gen = images_mpgen_all[i:i+128]
            batch_mp = images_mp_all[i:i+128]

            with torch.no_grad():
                mask_input_states, input_states, segments, length_masks, input_masks = prepare_batch(batch_gt)
                output_gt = sketchbert(mask_input_states, length_masks, segments=segments, head_mask=None)[:,0]
                output_gt /= torch.norm(output_gt)

                mask_input_states, input_states, segments, length_masks, input_masks = prepare_batch(batch_gen)
                output_gen = sketchbert(mask_input_states, length_masks, segments=segments, head_mask=None)[:,0]
                output_gen /= torch.norm(output_gen)

                mask_input_states, input_states, segments, length_masks, input_masks = prepare_batch(batch_mp)
                output_mp = sketchbert(mask_input_states, length_masks, segments=segments, head_mask=None)[:,0]
                output_mp /= torch.norm(output_mp)

            gt_vs_mp_sketchbert = (output_gt @ output_mp.T).diag().cpu().tolist()
            gt_vs_gen_sketchbert = (output_gt @ output_gen.T).diag().cpu().tolist()
            sketchbert_gt_mp.extend(gt_vs_mp_sketchbert)
            sketchbert_gt_gen.extend(gt_vs_gen_sketchbert)
        
        df['sketchbertMP'] = sketchbert_gt_mp
        df['sketchbertGen'] = sketchbert_gt_gen

    if 'clip' in metrics:
        
        model.eval()
        clip_aug = []
        clip_auggen = []
        clip_mp = []
        clip_mpgen = []

        for i in range(0,len(images_mpgen_all), batch_size):
            batch_gt = images_gt_all[i:i+batch_size]
            batch_gen = images_mpgen_all[i:i+batch_size]
            batch_mp = images_mp_all[i:i+batch_size]

            with torch.no_grad():
                gt_clip_inputs = processor(images=images_gt_all, return_tensors = 'pt').pixel_values.cuda()
                gt_outputs = model(gt_clip_inputs).image_embeds
                gt_outputs = gt_outputs / gt_outputs.norm(dim=1, keepdim=True)

                aug_clip_inputs = processor(images=images_aug_all, return_tensors = 'pt').pixel_values.cuda()
                aug_outputs = model(aug_clip_inputs).image_embeds
                aug_outputs = aug_outputs / aug_outputs.norm(dim=1, keepdim=True)

                mp_clip_inputs = processor(images=images_mp_all, return_tensors = 'pt').pixel_values.cuda()
                mp_outputs = model(mp_clip_inputs).image_embeds
                mp_outputs = mp_outputs / mp_outputs.norm(dim=1, keepdim=True)

                auggen_clip_inputs = processor(images=images_auggen_all, return_tensors = 'pt').pixel_values.cuda()
                auggen_outputs = model(auggen_clip_inputs).image_embeds
                auggen_outputs = auggen_outputs / auggen_outputs.norm(dim=1, keepdim=True)

                mpgen_clip_inputs = processor(images=images_mpgen_all, return_tensors = 'pt').pixel_values.cuda()
                mpgen_outputs = model(mpgen_clip_inputs).image_embeds
                mpgen_outputs = mpgen_outputs / mpgen_outputs.norm(dim=1, keepdim=True)  

                gt_vs_aug = (gt_outputs @ aug_outputs.T).diag().cpu().tolist()
                gt_vs_mp = (gt_outputs @ mp_outputs.T).diag().cpu().tolist()
                gt_vs_auggen = (gt_outputs @ auggen_outputs.T).diag().cpu().tolist()
                gt_vs_mpgen = (gt_outputs @ mpgen_outputs.T).diag().cpu().tolist()

                clip_aug.extend(gt_vs_aug)
                clip_mp.extend(gt_vs_mp)
                clip_auggen.extend(gt_vs_auggen)
                clip_mpgen.extend(gt_vs_mpgen)
        
        df['clipAug'] = clip_aug
        df['clipMP'] = clip_mp
        df['clipAugGen'] = clip_auggen
        df['clipMPGen'] = clip_mpgen

    df = pd.DataFrame(results, columns = columns)
    seen_category_df = df[~df['category'].isin(held_out_categories)]
    held_out_categories_df = df[df['category'].isin(held_out_categories)]
    
    print("Seen categories")
    print(seen_category_df[df.columns[3:]].mean())
    print("Held out categories")
    print(held_out_categories_df[df.columns[3:]].mean())
    if save_path:
        df.to_json(save_path, orient='records', lines=True)


if __name__ == "__main__":
    pass